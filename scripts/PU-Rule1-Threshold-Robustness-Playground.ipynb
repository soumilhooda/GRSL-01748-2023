{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatches for 2013:\n",
      "Truth value: PU_Rule1_Thresholding-Robustness-O-2013.csv\n",
      "To test against: PU_Rule1_Thresholding-Robustness-MS-2013.csv\n",
      "Label mismatch counts:\n",
      "{2.0: 23420}\n",
      "\n",
      "Truth value: PU_Rule1_Thresholding-Robustness-O-2013.csv\n",
      "To test against: PU_Rule1_Thresholding-Robustness-IS-2013.csv\n",
      "Label mismatch counts:\n",
      "{3.0: 186}\n",
      "\n",
      "Truth value: PU_Rule1_Thresholding-Robustness-O-2013.csv\n",
      "To test against: PU_Rule1_Thresholding-Robustness-LS-2013.csv\n",
      "Label mismatch counts:\n",
      "{3.0: 136}\n",
      "\n",
      "Truth value: PU_Rule1_Thresholding-Robustness-O-2013.csv\n",
      "To test against: PU_Rule1_Thresholding-Robustness-ELS-2013.csv\n",
      "Label mismatch counts:\n",
      "{3.0: 93}\n",
      "\n",
      "Truth value: PU_Rule1_Thresholding-Robustness-O-2013.csv\n",
      "To test against: PU_Rule1_Thresholding-Robustness-ELL-2013.csv\n",
      "Label mismatch counts:\n",
      "{2.0: 5}\n",
      "\n",
      "Truth value: PU_Rule1_Thresholding-Robustness-O-2013.csv\n",
      "To test against: PU_Rule1_Thresholding-Robustness-LL-2013.csv\n",
      "Label mismatch counts:\n",
      "{2.0: 121}\n",
      "\n",
      "Truth value: PU_Rule1_Thresholding-Robustness-O-2013.csv\n",
      "To test against: PU_Rule1_Thresholding-Robustness-IL-2013.csv\n",
      "Label mismatch counts:\n",
      "{2.0: 213}\n",
      "\n",
      "Truth value: PU_Rule1_Thresholding-Robustness-O-2013.csv\n",
      "To test against: PU_Rule1_Thresholding-Robustness-ML-2013.csv\n",
      "Label mismatch counts:\n",
      "{2.0: 340}\n",
      "\n",
      "Mismatches for 2020:\n",
      "Truth value: PU_Rule1_Thresholding-Robustness-O-2020.csv\n",
      "To test against: PU_Rule1_Thresholding-Robustness-MS-2020.csv\n",
      "Label mismatch counts:\n",
      "{2.0: 22428}\n",
      "\n",
      "Truth value: PU_Rule1_Thresholding-Robustness-O-2020.csv\n",
      "To test against: PU_Rule1_Thresholding-Robustness-IS-2020.csv\n",
      "Label mismatch counts:\n",
      "{3.0: 346}\n",
      "\n",
      "Truth value: PU_Rule1_Thresholding-Robustness-O-2020.csv\n",
      "To test against: PU_Rule1_Thresholding-Robustness-LS-2020.csv\n",
      "Label mismatch counts:\n",
      "{3.0: 261}\n",
      "\n",
      "Truth value: PU_Rule1_Thresholding-Robustness-O-2020.csv\n",
      "To test against: PU_Rule1_Thresholding-Robustness-ELS-2020.csv\n",
      "Label mismatch counts:\n",
      "{3.0: 188}\n",
      "\n",
      "Truth value: PU_Rule1_Thresholding-Robustness-O-2020.csv\n",
      "To test against: PU_Rule1_Thresholding-Robustness-ELL-2020.csv\n",
      "Label mismatch counts:\n",
      "{}\n",
      "\n",
      "Truth value: PU_Rule1_Thresholding-Robustness-O-2020.csv\n",
      "To test against: PU_Rule1_Thresholding-Robustness-LL-2020.csv\n",
      "Label mismatch counts:\n",
      "{2.0: 196}\n",
      "\n",
      "Truth value: PU_Rule1_Thresholding-Robustness-O-2020.csv\n",
      "To test against: PU_Rule1_Thresholding-Robustness-IL-2020.csv\n",
      "Label mismatch counts:\n",
      "{2.0: 349}\n",
      "\n",
      "Truth value: PU_Rule1_Thresholding-Robustness-O-2020.csv\n",
      "To test against: PU_Rule1_Thresholding-Robustness-ML-2020.csv\n",
      "Label mismatch counts:\n",
      "{2.0: 541}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of truth files and corresponding test files for 2013 and 2020\n",
    "truth_files_2013 = ['PU_Rule1_Thresholding-Robustness-O-2013.csv']\n",
    "test_files_2013 = ['PU_Rule1_Thresholding-Robustness-MS-2013.csv',\n",
    "                   'PU_Rule1_Thresholding-Robustness-IS-2013.csv',\n",
    "                   'PU_Rule1_Thresholding-Robustness-LS-2013.csv',\n",
    "                   'PU_Rule1_Thresholding-Robustness-ELS-2013.csv',\n",
    "                   'PU_Rule1_Thresholding-Robustness-ELL-2013.csv',\n",
    "                   'PU_Rule1_Thresholding-Robustness-LL-2013.csv',\n",
    "                   'PU_Rule1_Thresholding-Robustness-IL-2013.csv',\n",
    "                   'PU_Rule1_Thresholding-Robustness-ML-2013.csv']\n",
    "\n",
    "truth_files_2020 = ['PU_Rule1_Thresholding-Robustness-O-2020.csv']\n",
    "test_files_2020 = ['PU_Rule1_Thresholding-Robustness-MS-2020.csv',\n",
    "                   'PU_Rule1_Thresholding-Robustness-IS-2020.csv',\n",
    "                   'PU_Rule1_Thresholding-Robustness-LS-2020.csv',\n",
    "                   'PU_Rule1_Thresholding-Robustness-ELS-2020.csv',\n",
    "                   'PU_Rule1_Thresholding-Robustness-ELL-2020.csv',\n",
    "                   'PU_Rule1_Thresholding-Robustness-LL-2020.csv',\n",
    "                   'PU_Rule1_Thresholding-Robustness-IL-2020.csv',\n",
    "                   'PU_Rule1_Thresholding-Robustness-ML-2020.csv']\n",
    "\n",
    "def count_mismatches(truth_file, test_files):\n",
    "    truth_df = pd.read_csv(truth_file)\n",
    "    result = {}\n",
    "\n",
    "    for test_file in test_files:\n",
    "        test_df = pd.read_csv(test_file)\n",
    "        joined_df = truth_df.merge(test_df, on=['LAT', 'LON'], suffixes=('_truth', '_test'))\n",
    "        mismatched_rows = joined_df[joined_df['LABEL_truth'] != joined_df['LABEL_test']]\n",
    "        label_counts = mismatched_rows['LABEL_truth'].value_counts().to_dict()\n",
    "        result[test_file] = label_counts\n",
    "\n",
    "    return result\n",
    "\n",
    "# Calculate mismatches for 2013\n",
    "print(\"Mismatches for 2013:\")\n",
    "mismatches_2013 = count_mismatches(truth_files_2013[0], test_files_2013)\n",
    "for test_file, label_counts in mismatches_2013.items():\n",
    "    print(f\"Truth value: {truth_files_2013[0]}\")\n",
    "    print(f\"To test against: {test_file}\")\n",
    "    print(\"Label mismatch counts:\")\n",
    "    print(label_counts)\n",
    "    print()\n",
    "\n",
    "# Calculate mismatches for 2020\n",
    "print(\"Mismatches for 2020:\")\n",
    "mismatches_2020 = count_mismatches(truth_files_2020[0], test_files_2020)\n",
    "for test_file, label_counts in mismatches_2020.items():\n",
    "    print(f\"Truth value: {truth_files_2020[0]}\")\n",
    "    print(f\"To test against: {test_file}\")\n",
    "    print(\"Label mismatch counts:\")\n",
    "    print(label_counts)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
